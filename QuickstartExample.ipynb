{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook shows the MEP quickstart sample, which also exists as a non-notebook version at:\n",
    "https://bitbucket.org/vitotap/python-spark-quickstart\n",
    "\n",
    "It shows how to use Spark (http://spark.apache.org/) for distributed processing on the PROBA-V Mission Exploitation Platform. (https://proba-v-mep.esa.int/) The sample intentionally implements a very simple computation: for each PROBA-V tile in a given bounding box and time range, a histogram is computed. The results are then summed and printed. Computation of the histograms runs in parallel.\n",
    "## First step: get file paths\n",
    "A catalog API is available to easily retrieve paths to PROBA-V files:\n",
    "https://readthedocs.org/projects/mep-catalogclient/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'BioPar_ALB_BHV_V1_Tiles',\n",
       " u'BioPar_ALB_DHV_V1_Tiles',\n",
       " u'BioPar_BA_V1_Tiles',\n",
       " u'BioPar_DMP_Tiles',\n",
       " u'BioPar_FAPAR_V1_Tiles',\n",
       " u'BioPar_FCOVER_V1_Tiles',\n",
       " u'BioPar_LAI_V1_Tiles',\n",
       " u'BioPar_NDVI300_V1_Global',\n",
       " u'BioPar_BA300_V1_Global',\n",
       " u'BioPar_FCOVER300_V1_Global',\n",
       " u'BioPar_FAPAR300_V1_Global',\n",
       " u'BioPar_LAI300_V1_Global',\n",
       " u'BioPar_NDVI_V1_Tiles',\n",
       " u'BioPar_NDVI_V2_Tiles',\n",
       " u'BioPar_SWI',\n",
       " u'BioPar_SWI10_V3_Global',\n",
       " u'BioPar_TOCR_Tiles',\n",
       " u'BioPar_VCI_Tiles',\n",
       " u'BioPar_VPI_Tiles',\n",
       " u'BioPar_WB_V1_Tiles',\n",
       " u'BioPar_WB_V2_Tiles',\n",
       " u'PROBAV_L3_S1_TOC_1KM',\n",
       " u'PROBAV_L3_S1_TOC_333M',\n",
       " u'PROBAV_L3_S10_TOC_333M',\n",
       " u'PROBAV_L3_S5_TOC_100M',\n",
       " u'PROBAV_L3_S1_TOC_100M',\n",
       " u'PROBAV_L3_S10_TOC_1KM',\n",
       " u'PROBAV_L3_S1_TOA_1KM',\n",
       " u'PROBAV_L3_S1_TOA_333M',\n",
       " u'PROBAV_L3_S5_TOA_100M',\n",
       " u'PROBAV_L3_S1_TOA_100M',\n",
       " u'PROBAV_L3_S10_TOA_1KM',\n",
       " u'PROBAV_L3_S10_TOA_333M',\n",
       " u'PROBAV_L1C',\n",
       " u'SPOTVEGETATION_L3_S1',\n",
       " u'SPOTVEGETATION_L3_S10']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catalogclient import catalog\n",
    "cat=catalog.Catalog()\n",
    "cat.get_producttypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files.\n",
      "/data/MTDA/TIFFDERIVED/PROBAV_L3_S1_TOC_333M/2016/20160101/PROBAV_S1_TOC_20160101_333M_V101/PROBAV_S1_TOC_X18Y02_20160101_333M_V101_NDVI.tif\n",
      "/data/MTDA/TIFFDERIVED/PROBAV_L3_S1_TOC_333M/2016/20160101/PROBAV_S1_TOC_20160101_333M_V101/PROBAV_S1_TOC_X18Y02_20160101_333M_V101_NDVI.tif: TIFF image data, little-endian\r\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "date = datetime.date(2016, 1, 1)\n",
    "products = cat.get_products('PROBAV_L3_S1_TOC_333M', \n",
    "                            fileformat='GEOTIFF', \n",
    "                            startdate=date, \n",
    "                            enddate=date, \n",
    "                            min_lon=0, max_lon=10, min_lat=36, max_lat=53)\n",
    "#extract NDVI geotiff files from product metadata\n",
    "files = map(lambda p: filter(lambda f: 'NDVI' in f.bands,p.files)[0].filename[5:],products)\n",
    "print('Found '+str(len(files)) + ' files.')\n",
    "print(files[0])\n",
    "#check if file exists\n",
    "!file {files[0]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculates the histogram for a given (single band) image file.\n",
    "def histogram(image_file):\n",
    "    \n",
    "    import numpy as np\n",
    "    import gdal\n",
    "    \n",
    "    \n",
    "    # Open image file\n",
    "    img = gdal.Open(image_file)\n",
    "    \n",
    "    if img is None:\n",
    "        print '-ERROR- Unable to open image file \"%s\"' % image_file\n",
    "    \n",
    "    # Open raster band (first band)\n",
    "    raster = img.GetRasterBand(1)    \n",
    "    xSize = img.RasterXSize\n",
    "    ySize = img.RasterYSize\n",
    "    \n",
    "    # Read raster data\n",
    "    data = raster.ReadAsArray(0, 0, xSize, ySize)\n",
    "        \n",
    "    # Calculate histogram\n",
    "    hist, _ = np.histogram(data, bins=256)\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# === Calculate the histogram for a given number of files. The ===\n",
    "# === processing is performed by spreading them over a cluster ===\n",
    "# === of Spark nodes.                                          ===\n",
    "# ================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "import pyspark\n",
    "\n",
    "# Setup the Spark cluster\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.yarn.executor.memoryOverhead', 1024)\n",
    "conf.set('spark.executor.memory', '8g')\n",
    "conf.set('spark.executor.cores', '2')\n",
    "conf.set('spark.executor.instances', 10)\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of 2 histograms: [654193, 63823, 61682, 68119, 67839, 72141, 74221, 77644, 81447, 86393, 89683, 103483, 110195, 121177, 136138, 154763, 184321, 220988, 275457, 361887, 542747, 684021, 900909, 1174650, 1365573, 1156187, 841369, 645124, 510698, 424658, 354549, 313612, 280477, 255020, 235359, 224484, 211826, 199581, 184397, 173696, 165003, 155257, 148290, 142473, 137691, 133740, 130434, 127275, 125257, 122274, 121166, 118215, 116689, 114504, 113817, 110121, 108602, 106677, 102750, 101537, 98107, 96552, 94591, 92763, 90640, 87984, 85681, 84069, 81654, 79944, 78599, 76273, 74443, 73064, 70554, 69129, 67624, 66310, 64458, 64226, 62432, 61680, 59466, 60326, 58542, 57456, 56427, 55700, 54616, 53549, 53763, 52796, 51371, 51553, 50544, 49812, 49756, 49236, 49127, 48600, 47761, 48000, 47598, 47314, 46795, 47192, 47040, 46851, 46362, 46575, 46373, 45937, 46420, 46148, 45801, 45751, 45996, 45653, 45130, 45838, 45178, 45157, 45360, 45079, 44892, 44597, 44721, 44171, 43783, 43735, 43649, 43264, 42982, 42378, 42210, 41784, 41422, 41014, 40530, 40057, 39918, 39100, 38516, 38399, 37210, 36783, 36496, 36179, 35375, 35028, 34500, 33610, 32975, 32888, 32145, 31964, 30972, 29963, 29870, 29286, 28594, 28096, 27359, 26752, 26418, 25925, 25132, 24701, 24090, 23669, 22812, 22344, 21753, 21567, 20736, 20444, 19873, 19304, 18744, 17972, 17787, 17104, 16652, 16021, 15674, 14719, 14249, 13781, 13490, 12968, 12543, 12059, 11547, 11160, 10678, 9899, 9573, 9299, 8868, 8348, 7838, 7620, 7181, 6737, 6372, 6019, 5867, 5049, 5476, 4789, 4443, 4204, 3934, 3830, 3392, 3160, 2908, 2712, 2478, 2465, 2149, 1974, 1828, 1677, 1559, 1506, 1338, 1223, 1175, 1029, 1030, 867, 812, 728, 653, 672, 574, 566, 559, 436, 439, 401, 402, 371, 338, 297, 319, 286, 283, 269, 8589, 0, 0, 0, 0, 308278]\n"
     ]
    }
   ],
   "source": [
    "# Distribute the local file list over the cluster.\n",
    "filesRDD = sc.parallelize(files)\n",
    "\n",
    "# Apply the 'histogram' function to each filename using 'map', keep the result in memory using 'cache'.\n",
    "hists = filesRDD.map(histogram).cache()\n",
    "\n",
    "count = hists.count()\n",
    "\n",
    "# Combine distributed histograms into a single result\n",
    "total = hists.reduce(lambda h, i: map(add, h, i))\n",
    "\n",
    "print \"Sum of %i histograms: %s\" % (count, total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
