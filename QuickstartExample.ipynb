{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of files for which a histogram needs to be calculated. Each file should be a single-band file\n",
    "# supported by GDAL.\n",
    "\n",
    "files = [\n",
    "    \"/data/MTDA/TIFFDERIVED/PROBAV_L3_S1_TOC_333M/20160101/PROBAV_S1_TOC_20160101_333M_V001/PROBAV_S1_TOC_X18Y02_20160101_333M_V001_NDVI.tif\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates the histogram for a given (single band) image file.\n",
    "def histogram(image_file):\n",
    "    \n",
    "    import numpy as np\n",
    "    import gdal\n",
    "    \n",
    "    # Open image file\n",
    "    img = gdal.Open(image_file)\n",
    "    \n",
    "    if not img:\n",
    "        print '-ERROR- Unable to open image file \"%s\"' % image_file\n",
    "    \n",
    "    # Open raster band (first band)\n",
    "    raster = img.GetRasterBand(1)    \n",
    "    xSize = raster.RasterXSize\n",
    "    ySize = raster.RasterYSize\n",
    "    \n",
    "    # Read raster data\n",
    "    data = raster.ReadAsArray(0, 0, xSize, ySize)\n",
    "        \n",
    "    # Calculate histogram\n",
    "    hist, _ = np.histogram(band, bins=256)\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# === Calculate the histogram for a given number of files. The ===\n",
    "# === processing is performed by spreading them over a cluster ===\n",
    "# === of Spark nodes.                                          ===\n",
    "# ================================================================\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from datetime import datetime\n",
    "from operator import add\n",
    "\n",
    "# Setup the Spark cluster\n",
    "conf = SparkConf()\n",
    "conf.set('spark.yarn.executor.memoryOverhead', 1024)\n",
    "conf.set('spark.executor.memory', '8g')\n",
    "conf.set('spark.executor.cores', '2')\n",
    "conf.set('spark.executor.instances', 10)\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# Distribute the local file list over the cluster.\n",
    "filesRDD = sc.parallelize(files)\n",
    "\n",
    "# Apply the 'histogram' function to each filename using 'map', keep the result in memory using 'cache'.\n",
    "hists = filesRDD.map(histogram).cache()\n",
    "\n",
    "count = hists.count()\n",
    "\n",
    "# Combine distributed histograms into a single result\n",
    "total = hists.reduce(lambda h, i: map(add, h, i))\n",
    "\n",
    "print \"Sum of %i histograms: %s\" % (count, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
